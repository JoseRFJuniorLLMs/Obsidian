[[Accuracy]]: The ratio of correct predictions (true positives + true negatives) to the total number of predictions. In other words, what proportion of diabetes predictions did the model get right? • [[Precision]]: Precision is a measure of the correct positive results. Precision is the number of true positives divided by the sum of the number of true positives and false positives. Precision is scored between 0 and 1. Values closer to 1 are better. • 
[[Recall]]: The fraction of the cases classified as positive that are actually positive (the number of true positives divided by the number of true positives plus false negatives). In other words, out of all the patients who actually have diabetes, how many did the model identify? • 
[[F1 Score]]: F1 score is a measure combining precision and recall. F1 score is the weighted average of precision and recall (the number of true positives divided by the sum of true positives and false negatives). F-score is scored between 0 and 1. Values closer to 1 are better. • 
[[AUC]]: measures the area under a curve that represents true positive rate over true negative rate. AUC ranges between 0 and 1. Values closer to 1 indicate that the model is performing better. AUC value of 0.4 means that the model is performing worse than a random guess. AUC values range between 0 and 1. The higher the value, the better the performance of the classification model.